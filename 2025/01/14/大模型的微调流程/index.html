<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型的微调流程 | ShangYiYong</title><meta name="author" content="Shang YiYong"><meta name="copyright" content="Shang YiYong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="首先，大模型微调（Fine-tuning）应该是指在预训练好的模型基础上，针对特定任务或领域的数据进行进一步训练，使模型适应新的任务。那流程大致包括几个步骤呢？1 确定任务和目标：首先需要明确微调的任务是什么，比如文本分类、问答系统等，以及希望模型达到什么样的效果。2 准备数据：收集和整理与任务相关的数据集，可能需要标注数据，然后划分训练集、验证集和测试集。3 数据预处理：将数据转换成模型可以接受">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型的微调流程">
<meta property="og:url" content="https://ericshang98.github.io/2025/01/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B/index.html">
<meta property="og:site_name" content="ShangYiYong">
<meta property="og:description" content="首先，大模型微调（Fine-tuning）应该是指在预训练好的模型基础上，针对特定任务或领域的数据进行进一步训练，使模型适应新的任务。那流程大致包括几个步骤呢？1 确定任务和目标：首先需要明确微调的任务是什么，比如文本分类、问答系统等，以及希望模型达到什么样的效果。2 准备数据：收集和整理与任务相关的数据集，可能需要标注数据，然后划分训练集、验证集和测试集。3 数据预处理：将数据转换成模型可以接受">
<meta property="og:locale">
<meta property="og:image" content="https://ericshang98.github.io/images/avatar2.jpg">
<meta property="article:published_time" content="2025-01-14T03:19:32.000Z">
<meta property="article:modified_time" content="2025-05-14T04:38:54.087Z">
<meta property="article:author" content="Shang YiYong">
<meta property="article:tag" content="尚奕勇,个人空间,生活,工作,学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ericshang98.github.io/images/avatar2.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大模型的微调流程",
  "url": "https://ericshang98.github.io/2025/01/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B/",
  "image": "https://ericshang98.github.io/images/avatar2.jpg",
  "datePublished": "2025-01-14T03:19:32.000Z",
  "dateModified": "2025-05-14T04:38:54.087Z",
  "author": [
    {
      "@type": "Person",
      "name": "Shang YiYong",
      "url": "https://ericshang98.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ericshang98.github.io/2025/01/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型的微调流程',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/avatar2.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-list"></i><span> List</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ShangYiYong</span></a><a class="nav-page-title" href="/"><span class="site-name">大模型的微调流程</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-list"></i><span> List</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大模型的微调流程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-01-14T03:19:32.000Z" title="Created 2025-01-14 11:19:32">2025-01-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-05-14T04:38:54.087Z" title="Updated 2025-05-14 12:38:54">2025-05-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>8mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>首先，大模型微调（Fine-tuning）应该是指在预训练好的模型基础上，针对特定任务或领域的数据进行进一步训练，使模型适应新的任务。那流程大致包括几个步骤呢？<br>1 确定任务和目标：首先需要明确微调的任务是什么，比如文本分类、问答系统等，以及希望模型达到什么样的效果。<br>2 准备数据：收集和整理与任务相关的数据集，可能需要标注数据，然后划分训练集、验证集和测试集。<br>3 数据预处理：将数据转换成模型可以接受的格式，比如tokenization、添加特殊标记（如[CLS]、[SEP]）等。<br>4 选择预训练模型：根据任务选择合适的预训练模型，如BERT、GPT、T5等。<br>5 调整模型结构：可能需要在预训练模型的基础上添加一些任务特定的层，比如分类任务加一个全连接层。 6 设置训练参数：确定学习率、批次大小、训练轮数（epochs）、优化器（如AdamW）、权重衰减、学习率调度等超参数。<br>7 训练过程：在训练数据上进行微调，同时监控验证集的损失和指标，防止过拟合。<br>8 评估与验证：使用测试集评估模型性能，分析结果，可能需要调整超参数或数据。<br>9 部署与应用：将微调后的模型部署到生产环境，持续监控性能，可能需要迭代优化。</p>
<p><img src="/images/1747193308558.jpg" alt="alt text"></p>
<p>不过过程中还有一些细节问题，比如数据预处理可能需要更详细的步骤，比如处理不平衡数据、数据增强等。另外，可能还需要考虑计算资源，比如使用GPU还是TPU，分布式训练等。 </p>
<p>另外，微调的方法也有多种，比如全参数微调（full fine-tuning）和参数高效微调方法（如LoRA、Adapter、Prefix-tuning），这些可能在调整模型结构或训练参数时需要提到。<br>还有可能需要注意是否要冻结部分层，只训练特定层，这样可以减少计算量，但可能影响效果。这时候需要在调整模型结构时决定哪些层需要冻结。<br>此外，训练过程中可能需要早停（early stopping），根据验证集的表现来提前终止训练，避免过拟合。还有梯度裁剪（gradient clipping）来防止梯度爆炸。 </p>
<p>在评估阶段，除了准确率、F1值等指标，可能还要考虑模型在不同子集上的表现，比如不同类别的数据，或者进行错误分析，找出模型在哪些情况下表现不佳。 部署阶段可能需要将模型转换为适合生产环境的格式，比如ONNX，或者使用特定的框架如TensorFlow Serving、TorchServe等。<br>此外，还需要考虑模型的版本控制和监控日志等。 可能还需要提到持续学习或在线学习，但通常在微调中可能是一次性的，除非有持续的数据更新。 </p>
<h2 id="面试技巧"><a href="#面试技巧" class="headerlink" title="面试技巧"></a>面试技巧</h2><p>那在面试中，应该如何结构清晰地回答这个问题呢？应该按照步骤顺序，简明扼要地说明每个阶段的关键点，同时可以加入一些细节来展示自己的深度。<br>比如，当提到数据准备时，可以强调数据质量和多样性对微调效果的重要性；在模型选择时，说明不同模型架构的适用场景；在训练参数设置时，提到学习率通常较小，因为预训练模型已经在大规模数据上训练过，参数只需要微调；或者使用分层学习率，不同层的参数更新速度不同。</p>
<p>另外，可能还要提到一些常见的问题和解决方案，比如过拟合时可以通过增加正则化（如dropout）、数据增强、减少模型复杂度等；或者训练过程中的硬件限制，比如批次大小的调整、梯度累积等技巧。</p>
<p>现在，总结一下可能的回答框架：<br>1 明确任务与数据准备<br>2 数据预处理与划分<br>3 选择并加载预训练模型<br>4 调整模型结构以适应任务<br>5 设置超参数与优化策略<br>6 训练与验证监控<br>7 模型评估与迭代优化<br>8 部署与维护</p>
<p>在每一步中，需要具体说明做什么，为什么这样做，以及注意事项。<br>例如，在数据预处理中，要确保输入格式符合模型要求，处理缺失值或噪声数据；在模型结构调整中，添加任务层或冻结部分层等。 可能还需要提到不同的微调方法，例如全参数微调与参数高效微调的区别，以及适用场景，比如当计算资源有限时使用LoRA等方法。 最后，可以结合实际案例，比如在文本分类任务中微调BERT模型的流程，这样可以让回答更具体，展示实际经验。</p>
<p>现在检查是否有遗漏的步骤，比如模型保存与版本管理，或者在部署后的模型监控和更新策略。此外，可能还需要考虑评估指标的选择，根据任务的不同选择合适的评估方法。<br>另外，在数据预处理阶段，可能需要数据增强，比如回译、随机删除、替换等，尤其是当训练数据不足时。或者使用领域自适应的方法，如果目标领域的数据分布与预训练数据不同。 在训练过程中，分布式训练和混合精度训练（如使用FP16）也是常见的加速训练的手段，可以在设置训练参数时提到。<br>还有，微调后的模型可能需要量化或剪枝以优化推理速度，特别是在资源受限的环境中，这可能在部署阶段需要考虑。<br>总之，回答时需要覆盖整个流程的关键步骤，同时根据时间限制选择是否深入某些技术细节，以展示自己的专业性和实践经验。</p>
<p>在面试中回答大模型微调流程时，建议采用结构化的方式，突出关键步骤和技术细节。以下是一个清晰且专业的回答框架：</p>
<h2 id="大模型微调流程"><a href="#大模型微调流程" class="headerlink" title="大模型微调流程"></a>大模型微调流程</h2><p>大模型微调（Fine-tuning）是在预训练模型的基础上，通过特定领域或任务的数据进行针对性训练，以提升模型在目标场景中的性能。其核心流程如下：</p>
<p><img src="/images/1747193932777.jpg" alt="alt text"></p>
<ol>
<li><p>任务定义与数据准备<br>○ 明确任务目标：确定微调任务类型（如文本分类、实体识别、生成任务等），明确评估指标（如准确率、F1值、BLEU等）。<br>○ 数据收集与标注：收集与任务相关的数据，确保数据质量和多样性；对非监督任务可能需要标注（如分类标签、实体标签）。<br>○ 数据划分：将数据分为训练集、验证集和测试集（常用比例如80%-10%-10%），验证集用于调参，测试集用于最终评估。</p>
</li>
<li><p>数据预处理与增强<br>○ 格式转换：将数据转换为模型输入格式（如BERT的[CLS]+文本+[SEP]）。<br>○ 分词（Tokenization）：使用预训练模型的对应分词器（如BERT的WordPiece）。<br>○ 数据增强：在数据量不足时，采用回译、随机遮盖、同义词替换等方法扩充数据，提升泛化性。</p>
</li>
<li><p>模型选择与结构调整<br>○ 选择预训练模型：根据任务选择合适的基础模型（如BERT用于理解任务，GPT-3用于生成任务）。<br>○ 调整模型结构：<br> ■ 全参数微调：直接更新所有参数，适用于数据充足且任务差异较大的场景。<br> ■ 参数高效微调：如LoRA（低秩适应）、Adapter（插入适配层），仅训练少量参数，节省计算资源。<br> ■ 添加任务层：例如在分类任务中，添加全连接层输出类别概率。</p>
</li>
<li><p>训练超参数与优化策略<br>○ 学习率设置：通常使用较小的学习率（如1e-5到5e-5），避免破坏预训练特征。<br>○ 优化器选择：常用AdamW，配合权重衰减（Weight Decay）防止过拟合。<br>○ 训练技巧：<br> ■ 分层学习率：不同层设置不同学习率（底层参数更新更慢）。<br> ■ 梯度裁剪：防止梯度爆炸。<br> ■ 早停（Early Stopping）：根据验证集表现终止训练。<br> ■ 混合精度训练：使用FP16加速训练，节省显存。</p>
</li>
<li><p>训练与验证监控<br>○ 训练过程：在训练集上迭代更新参数，同时监控训练损失和验证集指标。<br>○ 防止过拟合：通过Dropout、数据增强、正则化（L2）等手段提升泛化能力。</p>
</li>
<li><p>模型评估与迭代优化<br>○ 测试集评估：使用保留的测试集计算最终指标，确保模型未过拟合验证集。<br>○ 错误分析：分析模型在特定样本上的错误，针对性优化数据或模型结构。<br>○ 超参数调优：通过网格搜索或工具（如Optuna）优化学习率、批次大小等。</p>
</li>
<li><p>模型部署与持续迭代<br>○ 模型导出：将模型转换为生产格式（如ONNX、TensorRT）以提升推理效率。<br>○ 监控与更新：部署后监控线上表现，定期用新数据重新微调（持续学习）。<br>○ 轻量化处理：通过量化（Quantization）、剪枝（Pruning）压缩模型，适应边缘设备</p>
</li>
</ol>
<h2 id="技术细节补充（可选）"><a href="#技术细节补充（可选）" class="headerlink" title="技术细节补充（可选）"></a>技术细节补充（可选）</h2><p>● 参数高效微调：可提及LoRA（冻结原模型，添加低秩矩阵）、Prefix-Tuning（在输入前添加可训练前缀）等方法。<br>● 领域自适应：若目标领域数据分布与预训练数据差异大，可采用领域预训练（Domain-Adaptive Pretraining）后再微调。<br>● 多任务学习：同时微调多个相关任务，共享底层参数，提升泛化性。</p>
<h2 id="示例回答"><a href="#示例回答" class="headerlink" title="示例回答"></a>示例回答</h2><p>“大模型微调的流程通常从明确任务目标开始，例如分类或生成任务，接着收集和预处理相关数据。数据需要被分词并转换为模型输入格式，如添加特殊标记。之后，选择合适的预训练模型（如BERT或GPT），并根据任务调整结构，比如添加分类层。训练时采用较小的学习率和优化策略（如AdamW），同时监控验证集指标以防止过拟合。最后，通过测试集评估模型，并部署到生产环境，持续迭代优化。”</p>
<p>通过结构化表达和关键技术点（如参数高效微调、分层学习率）的提及，可以展示对微调流程的深入理解，同时结合实际场景（如数据不足时的增强方法）体现实践经验。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://ericshang98.github.io">Shang YiYong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://ericshang98.github.io/2025/01/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B/">https://ericshang98.github.io/2025/01/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/images/avatar2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/10/14/Transformer%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="Transformer多头自注意力机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Transformer多头自注意力机制</div></div><div class="info-2"><div class="info-item-1">解释多头注意力机制（Multi-Head Attention）需把握五个核心维度：  基本结构：并行注意力头的设计与融合 数学表达：Q&#x2F;K&#x2F;V矩阵变换与注意力计算 核心优势：多视角特征捕捉与模型表征能力 参数控制：头数选择与维度分割策略 工程实现：矩阵并行计算与内存优化  首先，多头注意力是Transformer架构的核心组件，允许模型同时关注不同位置的信息，从多个子空间捕获不同的特征。关键点包括：分头处理、线性变换、缩放点积注意力、多头融合以及残差连接和归一化。 接下来，可以从基本结构、数学表达、核心优势、应用场景这四个维度切入，这样结构清晰，容易让面试官理解。然后，结合真实项目案例。比如，在智能客服项目中优化意图识别模块，使用多头注意力机制。需要分点说明问题背景、解决方案的具体步骤，比如分头策略、参数配置、融合方式、训练技巧以及效果验证。这里要具体，用数据和实际结果来支撑。...</div></div></div></a><a class="pagination-related" href="/2025/03/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E9%9D%A2%E8%AF%95%E9%A2%981/" title="大模型相关面试题 1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">大模型相关面试题 1</div></div><div class="info-2"><div class="info-item-1">应用是AI非常重要的发展方向，了解底层原理才能做出好的产品。大模型相关的面试问题通常涉及模型的原理、应用、优化以及面试者对于该领域的理解和经验。 以下是一些从各种地方搜罗的常见的大模型面试问题以及建议的回答方式： 1、请简述什么是大模型，以及它与传统模型的主要区别是什么？大模型通常指的是参数数量巨大的深度学习模型，如 GPT 系列。它们与传统模型的主要区别在于规模：大模型拥有更多的参数和更复杂的结构，从而能够处理更复杂、更广泛的任务。此外，大模型通常需要更多的数据和计算资源进行训练和推理。 2、谈谈你对 Transformer 模型的理解，以及它在自然语言处理中的应用。Transformer 模型是一种基于自注意力机制的神经网络结构，它通过多头自注意力和编码器-解码器结构，有效地捕捉序列数据中的长期依赖关系。在自然语言处理中，Transformer 广泛应用于机器翻译、文本摘要、问答系统等任务，并取得了显著的性能提升。 3、你如何评估大模型的性能？有哪些常用的评估指标？评估大模型性能时，我们通常会考虑多个方面，包括准确率、召回率、F1...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar2.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Shang YiYong</div><div class="author-info-description">记录生活，记录工作，记录学习</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/ericshang98" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:shangyiyong@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E6%8A%80%E5%B7%A7"><span class="toc-number">1.</span> <span class="toc-text">面试技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">大模型微调流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E8%A1%A5%E5%85%85%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">技术细节补充（可选）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E5%9B%9E%E7%AD%94"><span class="toc-number">4.</span> <span class="toc-text">示例回答</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/15/Manus%E7%9A%84%E5%A4%9AAgent%E6%9E%B6%E6%9E%84%E6%80%8E%E4%B9%88%E5%81%9A%EF%BC%9F/" title="Manus的多Agent架构怎么做？">Manus的多Agent架构怎么做？</a><time datetime="2025-05-15T05:30:54.000Z" title="Created 2025-05-15 13:30:54">2025-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/AG-UI%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" title="AG-UI使用指南"><img src="/images/AGUI.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AG-UI使用指南"/></a><div class="content"><a class="title" href="/2025/05/13/AG-UI%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" title="AG-UI使用指南">AG-UI使用指南</a><time datetime="2025-05-13T12:48:47.000Z" title="Created 2025-05-13 20:48:47">2025-05-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/13/%E7%8C%AB%E5%92%AA%E5%91%BC%E5%99%9C%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/" title="猫咪呼噜算法解析"><img src="/images/moflin.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="猫咪呼噜算法解析"/></a><div class="content"><a class="title" href="/2025/05/13/%E7%8C%AB%E5%92%AA%E5%91%BC%E5%99%9C%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/" title="猫咪呼噜算法解析">猫咪呼噜算法解析</a><time datetime="2025-05-13T06:11:57.000Z" title="Created 2025-05-13 14:11:57">2025-05-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/11/Langgraph%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E7%AE%A1%E7%90%86-%E8%AE%B0%E5%BF%86%E6%A8%A1%E5%9D%97/" title="Langgraph的持久化管理-记忆模块">Langgraph的持久化管理-记忆模块</a><time datetime="2025-05-11T04:39:33.000Z" title="Created 2025-05-11 12:39:33">2025-05-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/10/Langgraph%E7%9A%84%E9%AD%85%E5%8A%9B/" title="Langgraph的魅力"><img src="/images/bg3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Langgraph的魅力"/></a><div class="content"><a class="title" href="/2025/05/10/Langgraph%E7%9A%84%E9%AD%85%E5%8A%9B/" title="Langgraph的魅力">Langgraph的魅力</a><time datetime="2025-05-10T07:10:44.000Z" title="Created 2025-05-10 15:10:44">2025-05-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Shang YiYong</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>